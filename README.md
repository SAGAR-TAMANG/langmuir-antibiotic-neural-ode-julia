# Langmuir-Antibiotic Neural ODE (Julia)


The Langmuir adsorption model is described by the following differential equation:

$$
\frac{dq}{dt} = k_a \cdot C \cdot (Q_m - q) - k_d \cdot q
$$

<p align="center">
  <img src="assets/trajectory_comparison.png"  alt="Trajectory (early)" width="40%">
  <img src="assets/trajectory_comparison_2.png" alt="Trajectory (final)" width="40%">
</p>

A **minimal yet complete Scientific-ML example** showing that a small Neural ODE can reproduce the classical Langmuir adsorption kinetics for antibiotics with sub-percent error.  
The project accompanies the paper <cite title to appear> and provides everything needed to:

- generate synthetic Langmuir data  
- train a Lux‚Äìbased Neural ODE with an ADAM ‚Üí BFGS pipeline  
- reproduce all plots, tables and final metrics reported in the manuscript  
- rebuild the LaTeX PDF

> **Repo URL:** <https://github.com/SAGAR-TAMANG/langmuir-antibiotic-neural-ode-julia>

---

## üìÇ Repository layout

```text
.
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ julia.ipynb        # solves the Langmuir ODE
‚îú‚îÄ assets/                      # generated PNGs (loss curve, trajectory comparison ‚Ä¶)
‚îú‚îÄ paper/
‚îÇ  ‚îú‚îÄ Neural ODE Surrogate for Langmuir Kinetics.zip   # arXiv‚Äêstyle LaTeX source
‚îú‚îÄ Project.toml               # Julia project manifest (Julia 1.10+)
‚îú‚îÄ Manifest.toml              # exact package versions
‚îú‚îÄ README.md                  # you are here
‚îî‚îÄ LICENSE                    # MIT
```

---

## üöÄ Quick start

1. **Clone and instantiate** (Julia 1.10+):

   ```bash
   git clone https://github.com/SAGAR-TAMANG/langmuir-antibiotic-neural-ode-julia.git
   cd langmuir-antibiotic-neural-ode-julia
   julia --project -e 'using Pkg; Pkg.instantiate()'
   ```

2. **Launch the notebook**

   Open `src/julia.ipynb` in [JupyterLab](https://jupyter.org/) or VSCode.

   > Make sure to select the Julia kernel (matching this repo's environment).

3. **Run all cells**

   The notebook contains everything: Langmuir data generation, Neural ODE training (ADAM + BFGS), plots, and exported figures.

---

## ‚ú® Key results (default run)

| Stage          | MSE          | RMSE   |
| -------------- |-------------:|-------:|
| Initial random | 4.07 e+02    | 20.2   |
| After **ADAM** | 2.52 e-03    | 0.050  |
| After **BFGS** | 5.16 e-06    | 0.0023 |

<figure>
  <img src="assets/loss_curve.jpg" width="100%" alt="loss curve">
  <figcaption align="center"><b>Figure ‚Äì</b> Loss curve of the training.</figcaption>
</figure>

---

## üõ†Ô∏è Main Julia packages

- [`DifferentialEquations.jl`](https://github.com/SciML/DifferentialEquations.jl) ‚Äì ODE solver (Tsit5)  
- [`DiffEqFlux.jl`](https://github.com/SciML/DiffEqFlux.jl) ‚Äì Neural ODE wrapper  
- [`Lux.jl`](https://github.com/LuxDL/Lux.jl) ‚Äì pure-function neural networks  
- [`ComponentArrays.jl`](https://github.com/SciML/ComponentArrays.jl) ‚Äì flat + named parameter containers  
- [`Optimization.jl`](https://github.com/SciML/Optimization.jl) ‚Äì ADAM, BFGS optimisers

Fully specified versions live in `Manifest.toml`.

---

## üìá Re-using the template

The code was written to be hackable:

* **Change network depth / width** in the notebook (`hidden_dim = 16`).
* **Add noise** in data cell (`ode_data .= ode_data .+ œÉ .* randn.`).
* **Swap optimiser** (`ADAM` ‚Üí `RMSProp`, etc.) via `Optimization.solve`.

---

## üìô Citation (Not Published Yet)

```bibtex
@misc{langmuir2025neuralode,
  author  = {Sagar Tamang},
  title   = {Neural Ordinary Differential Equations Reproduce Langmuir Antibiotic-Adsorption Kinetics},
  year    = {2025},
  url     = {https://github.com/SAGAR-TAMANG/langmuir-antibiotic-neural-ode-julia},
  note    = {GitHub repository}
}
```

---

## üìù License

This project is released under the **MIT License** ‚Äì see [`LICENSE`](LICENSE) for details. Contributions welcome via pull requests.

---

Happy modelling! üéâ
